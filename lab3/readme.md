# Список файлов и их краткое описание

* docker-compose.yaml - docker compose файл, содержащий все необходимые сервисы для работы (kakfa+zookeeper+kafka-ui и т.д.)
* deploy-jdbc-sink.sh - bash скрипт для создания топика, а так же вывода информации о созданном топике
* prometheus.yaml - Содержит конфиг для агентов prometheus
* plugins - папка с plugins для kafka-connect
* grafana - папка с конфигами и дашбордом для Grafana
* kafka-connect - папка содержит кофиги (jdbc-source, debezium connect) для kafka-connect, а так же конфиги и экспортер JMX метрик.   
* prometheus-metrics - папка содержит все необходимо для реализации сервиса вычитки из кафки сообщений, и предоствление из через api для агентов prometheus:
  - producer_metrics.py - простой продюсер, для отправки сообщений в топик кафку - эмуляция генерации метрик
  - templating.py - небольшой модуль для преобразования сообщений из кафка в формат prometheus
  - prometheus_metrics.py - основной модуль сервиса в рамках которого реализован вебсервер и несколькими endpoints
  - Dockerfile - для создания образа разраотанного сервиса, чтобы он разворачивался вместе со всеми компонентами
  - *.json - файлы с примерами из задания

# Описание

Для работы необходимо поднять docker compose:
```shell
docker compose up -d
```

Для добавления необходимых коннекторов в kafka-connector
```shell
./deploy-jdbc-sink.sh
```

Для начала отправки сообщений с метриками необходимо запустить producer_metrics.py
```shell
python producer_metrics.py
```

Для запуска сервиса по выгрузки сообщений из Kafka в Prometheus
```shell
python prometheus_metrics.py
```

Для настройки севрсиса необходимо отправить конфигурацию, котрая похожа на конфиг kafka-connect
```shell
curl -X POST -H "Content-Type:application/json" --data @prometheus-metrics/config.json http://localhost:8383/connectors
```
где http://localhost:8383 - это адрес и порт разработанного сервиса. Данный сервис поднят тоже через docker compose, чтобы все сервисы работали в единой сети

# Краткие замечания

* Так как реализовать через Python полноценный kafka коннектор нет возможности, то тут
получается реализация отдельного сервиса, внутри которого работает обычный кансамер, который вычитывает данные из топика при образщении на определенный endpoint.
* Многие вещи при реализации упрощены, т.к. это некторое PoC.



# Результаты эксперементов с ускорением JDBC Source

| Поле 1 | batch.max.rows | batch.size | linger.ms | Скорость записи |
---------|----------------|------------|-----------|-----------------|
| Поле 1 | 100            | 18400      | 1000      | 152k op/s       |
| Поле 1 | 1000           | 18400      | 1000      | 152k            |
| Поле 1 | 100            | 500        | 1000      | 19-20к             |
| Поле 1 | 1000           | 184000     | 1000      | 19к             |


# Лог с Debezium

Лги с запуском дебещиума в файле - logs_debezium.log

